# AI Gateway

A configurable API gateway for multiple LLM providers (OpenAI, Anthropic, Gemini, Ollama) with built-in analytics, guardrails, and administrative controls.

## Features Highlights

- **Multi-Provider Support**: Route requests to OpenAI, Anthropic, Gemini, Ollama, and Cohere
- **Automatic Failover**: When 2+ providers are configured, automatically fails over to alternative providers if primary provider fails
- **Rate Limiting**: Rate limiting policies
- **OpenAI compatible interface**: Standardized input and output based on OpenAI API inteface
- **Response Caching**: In-memory cache with configurable TTL for improved performance and reduced API costs
- **System Prompts**: Inject system prompts into all LLM requests
- **Response Guardrails**: Configure content filtering and response constraints
- **Analytics Dashboard**: Monitor usage, tokens, and errors with visual charts
- **Admin UI**: Configure AI gateway
- **Administrative Controls**: Configure gateway behavior via admin API

## HTTP API for chat completion

OpenAI compatible request interface
```shell
curl --location 'http://localhost:8080/v1/chat/completions' \
--header 'x-llm-provider: ollama' \
--header 'Content-Type: application/json' \
--data '{
  "messages": [{ 
        "role": "user",
        "content": "When will we have AGI? In 10 words" 
      }]
}
'
```

OpenAI API compatible response
```json
{
    "id": "01eff23c-208f-15a8-acdc-f400bba1bc6d",
    "object": "chat.completion",
    "created": 1740352553,
    "model": "llama3.1:latest",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "Estimating exact timeline uncertain, but likely within next few decades."
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 27,
        "completion_tokens": 14,
        "total_tokens": 41
    }
}
```

## gRPC API for chat completion

An example client in Python is available in `grpc-client` folder
```python
def run():
    # Create a gRPC channel
    channel = grpc.insecure_channel('localhost:8082')

    # Create a stub (client)
    stub = ai_gateway_pb2_grpc.AIGatewayStub(channel)

    # Create a request
    request = ai_gateway_pb2.ChatCompletionRequest(
        llm_provider="ollama",
        messages=[
            ai_gateway_pb2.Message(
                role="system",
                content="You are a helpful assistant."
            ),
            ai_gateway_pb2.Message(
                role="user",
                content="What is the capital of France?"
            )
        ]
    )

    try:
        # Make the call
        response = stub.ChatCompletion(request)

    # ...
```

## Switching between LLM providers

Use `x-llm-provider` HTTP header to route to different providers. AI Gateway mask request format differences between providers. Always use OpenAI API compatible request format and the gateway will always respond in OpenAI compatible response

| LLM Provider | Header name    | Header value |
| ------------ | -------------- | ------------ |
| OpenAI       | x-llm-provider | openai       |
| Ollama       | x-llm-provider | ollama       |
| Anthropic    | x-llm-provider | anthropic    |
| Gemini       | x-llm-provider | gemini       |
| Mistral      | x-llm-provider | mistral      |
| Cohere       | x-llm-provider | cohere       |

## Disable caching for requests

Gateway automatiacally enable response caching to improve performance and save costs. The default cache duration is 1 hour. If you specifically wants to disable caching for equests, then send `Cache-Control: no-cache` HTTP header with each request

## Gateway configuration

Gateway configuration can be done using either the built-in admin UI or using the REST API

### Admin UI

Main Admin UI display current stats on the server

<img width="1270" alt="admin-main" src="https://github.com/user-attachments/assets/d7d11794-f4c4-443c-8a4e-344fa286e024" />

Configure Settings: system prompt, guardrails, and clear cache

<img width="1274" alt="admin-settings" src="https://github.com/user-attachments/assets/f52c6b36-1e1c-46b0-a0d6-7f27e55a1a76" />

Add/modify logging config

<img width="1269" alt="admin-logging" src="https://github.com/user-attachments/assets/a1de999d-533c-4faa-ae3c-50e8567724dd" />

Add/modify rate limiting policy

<img width="1274" alt="admin-ratelimit" src="https://github.com/user-attachments/assets/5d34733c-d39a-4626-87fa-f085205b304c" />

### Add rate limiting

```shell
curl --location 'http://localhost:8081/admin/ratelimit' \
--header 'Content-Type: application/json' \
--data '{
    "name": "basic",
    "requestsPerWindow": 5,
    "windowSeconds": 60
  }'
```

Once rate limiting is enbaled, following 3 HTTP response headers will be used to announce current limits. These will be added to every HTTP response generated by the gateway

| Header name     | Value  | Description |
| --------------- | ------ | ----------- |
| RateLimit-Limit | number | Maximum number of requests allowed in the current policy |
| RateLimit-Remaining | number | Number of requests that can be sent before rate limit policy is enforced |
| RateLimit-Reset | number | How many seconds until current rate limit policy is reset

Following GET call will return the currently configured rate limiting policy. If the request is empty then rate limiting is disabled

```shell
curl --location 'http://localhost:8081/admin/ratelimit' \
--data ''
```
Respnose
```json
{
    "name": "basic",
    "requestsPerWindow": 5,
    "windowSeconds": 60
}
```

### Automatic failover

When 2 or more LLM providers are configured, the gateway will attempt automatic failover if there's no successful response from the provider user has chosen through `x-llm-provider` header.

The logs will dispaly a trail of failover like below. Here, the user is trying to send the request to Ollama. We have Ollama and OpenAI configured in the gateway.

First we can see a failed message. Following logs are formatted for clarity.
```
{
  "timestamp": "2025-02-24T00:33:51.127868Z",
  "level": "WARN",
  "component": "failover",
  "message": "Primary provider failed",
  "metadata": {
    "requestId": "01eff247-0444-1eb0-b153-61183107b722",
    "provider": "ollama",
    "error": "Something wrong with the connection:{}"
  }
}
```
First attempt to failover,
```
{
  "timestamp": "2025-02-24T00:33:51.129457Z",
  "level": "INFO",
  "component": "failover",
  "message": "Attempting failover",
  "metadata": {
    "requestId": "01eff247-0444-1eb0-b153-61183107b722",
    "provider": "openai"
  }
}
```

### System Prompt Injection ###

Admins can use the admin API to inject a system prompt for all out going requests. This will be appended to the system prompt if a user has supplied one in the request
```shell
curl --location 'http://localhost:8081/admin/systemprompt' \
--header 'Content-Type: application/json' \
--data '{
    "prompt": "respond only in chinese"
}'
```

Following GET request will show current system prompt
```shell
curl --location 'http://localhost:8081/admin/systemprompt'
```

### Enforcing guardrails ###

Use the following API call to add guardrails
```shell
curl --location 'http://localhost:8081/admin/guardrails' \
--header 'Content-Type: application/json' \
--data '{
    "bannedPhrases": ["obscene", "words"],
    "minLength": 0,
    "maxLength": 500000,
    "requireDisclaimer": false
}'
```

Get currently configured guardrails
```
curl --location 'http://localhost:8081/admin/guardrails' \
--data ''
```

### Cache Management

Gateway automatically enbale response caching for requests to save costs and enable responsiveness. Default cache duration is 1 hour. When requests are served from the cache, there will be a respective log printed to the logs.

The gateway will look for `Cache-Control: no-cache` header and will disable cache lookup for those requests

View current cached contents
```shell
curl --location 'http://localhost:8081/admin/cache'
```

Clear cache
```shell
curl --location --request DELETE 'http://localhost:8081/admin/cache'
```

## Configuration reference ##

Following is a complete example of all the configuration possible in the main gateway config file. At least one LLM provider config is mandatory

Create a `Config.toml` file:
```
[defaultLoggingConfig]
enableElasticSearch = false
elasticSearchEndpoint = "http://localhost:9200"
elasticApiKey = ""
enableSplunk = false
splunkEndpoint = ""
enableDatadog = false
datadogEndpoint = ""

[openAIConfig]
apiKey="your-api-key"
endpoint="https://api.openai.com"
model="gpt-4o"

[anthropicConfig]
apiKey="your-api-key"
model="claude-3-5-sonnet-20241022"
endpoint="https://api.anthropic.com"

[geminiConfig]
apiKey="your-api-key"
model="gemini-pro"
endpoint="https://generativelanguage.googleapis.com/v1/models"


[ollamaConfig]
apiKey=""
model="llama3.2"
endpoint="http://localhost:11434"

[mistralConfig]
apiKey = ""
model = "mistral-small-latest"
endpoint = "https://api.mistral.ai"

[cohereConfig]
apiKey = ""
model = "command-r-plus-08-2024"
endpoint = "https://api.cohere.com"
```

## Development

```bash
# Build and run the gateway
% bal run
```
